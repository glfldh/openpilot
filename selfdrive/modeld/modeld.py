#!/usr/bin/env python3
import os
from openpilot.system.hardware import TICI
WARP_DEVICE = 'QCOM' if TICI else 'CPU'
os.environ['DEV'] = WARP_DEVICE

USBGPU = "USBGPU" in os.environ
if USBGPU:
  os.environ['DEV'] = 'AMD'
  os.environ['AMD_IFACE'] = 'USB'
  os.environ['AMD_LLVM'] = '1'
  # perf
  os.environ['JIT_BATCH_SIZE'] = '0'
  os.environ['GRAPH_ONE_KERNEL'] = '1'
  os.environ['AMD_SDMA_BIND'] = '1'
MODEL_PREFIX = 'big_' if USBGPU else ''
from tinygrad.tensor import Tensor
from tinygrad import TinyJit, Device
import time
import pickle
import numpy as np
import cereal.messaging as messaging
from cereal import car, log
from pathlib import Path
from cereal.messaging import PubMaster, SubMaster
from msgq.visionipc import VisionIpcClient, VisionStreamType, VisionBuf
from opendbc.car.car_helpers import get_demo_car_params
from openpilot.common.swaglog import cloudlog
from openpilot.common.params import Params
from openpilot.common.filter_simple import FirstOrderFilter
from openpilot.common.realtime import config_realtime_process, DT_MDL
from openpilot.common.transformations.camera import DEVICE_CAMERAS
from openpilot.system.camerad.cameras.nv12_info import get_nv12_info
from openpilot.common.transformations.model import get_warp_matrix
from openpilot.selfdrive.controls.lib.desire_helper import DesireHelper
from openpilot.selfdrive.controls.lib.drive_helpers import get_accel_from_plan, smooth_value, get_curvature_from_plan
from openpilot.selfdrive.modeld.parse_model_outputs import Parser
from openpilot.selfdrive.modeld.fill_model_msg import fill_model_msg, fill_pose_msg, PublishState
from openpilot.selfdrive.modeld.constants import ModelConstants, Plan

from openpilot.selfdrive.modeld.compile_warp import make_frame_prepare
from openpilot.common.transformations.model import MEDMODEL_INPUT_SIZE

PROCESS_NAME = "selfdrive.modeld.modeld"
SEND_RAW_PRED = os.getenv('SEND_RAW_PRED')

VISION_METADATA_PATH = Path(__file__).parent / 'models/driving_vision_metadata.pkl'
POLICY_METADATA_PATH = Path(__file__).parent / 'models/driving_policy_metadata.pkl'
MODELS_DIR = Path(__file__).parent / 'models'

def policy_pkl_path(w, h):
  return MODELS_DIR / f'{MODEL_PREFIX}policy_{w}x{h}_tinygrad.pkl'

LAT_SMOOTH_SECONDS = 0.0
LONG_SMOOTH_SECONDS = 0.3
MIN_LAT_CONTROL_SPEED = 0.3

IMG_QUEUE_SHAPE = (6*(ModelConstants.MODEL_RUN_FREQ//ModelConstants.MODEL_CONTEXT_FREQ + 1), 128, 256)
assert IMG_QUEUE_SHAPE[0] == 30


def get_action_from_model(model_output: dict[str, np.ndarray], prev_action: log.ModelDataV2.Action,
                          lat_action_t: float, long_action_t: float, v_ego: float) -> log.ModelDataV2.Action:
    plan = model_output['plan'][0]
    desired_accel, should_stop = get_accel_from_plan(plan[:,Plan.VELOCITY][:,0],
                                                     plan[:,Plan.ACCELERATION][:,0],
                                                     ModelConstants.T_IDXS,
                                                     action_t=long_action_t)
    desired_accel = smooth_value(desired_accel, prev_action.desiredAcceleration, LONG_SMOOTH_SECONDS)

    desired_curvature = get_curvature_from_plan(plan[:,Plan.T_FROM_CURRENT_EULER][:,2],
                                                plan[:,Plan.ORIENTATION_RATE][:,2],
                                                ModelConstants.T_IDXS,
                                                v_ego,
                                                lat_action_t)
    if v_ego > MIN_LAT_CONTROL_SPEED:
      desired_curvature = smooth_value(desired_curvature, prev_action.desiredCurvature, LAT_SMOOTH_SECONDS)
    else:
      desired_curvature = prev_action.desiredCurvature

    return log.ModelDataV2.Action(desiredCurvature=float(desired_curvature),
                                  desiredAcceleration=float(desired_accel),
                                  shouldStop=bool(should_stop))

class FrameMeta:
  frame_id: int = 0
  timestamp_sof: int = 0
  timestamp_eof: int = 0

  def __init__(self, vipc=None):
    if vipc is not None:
      self.frame_id, self.timestamp_sof, self.timestamp_eof = vipc.frame_id, vipc.timestamp_sof, vipc.timestamp_eof

class InputQueues:
  def __init__ (self, model_fps, env_fps, n_frames_input):
    assert env_fps % model_fps == 0
    assert env_fps >= model_fps
    self.model_fps = model_fps
    self.env_fps = env_fps
    self.n_frames_input = n_frames_input

    self.dtypes = {}
    self.shapes = {}
    self.q = {}

  def update_dtypes_and_shapes(self, input_dtypes, input_shapes) -> None:
    self.dtypes.update(input_dtypes)
    if self.env_fps == self.model_fps:
      self.shapes.update(input_shapes)
    else:
      for k in input_shapes:
        shape = list(input_shapes[k])
        if 'img' in k:
          n_channels = shape[1] // self.n_frames_input
          shape[1] = (self.env_fps // self.model_fps + (self.n_frames_input - 1)) * n_channels
        else:
          shape[1] = (self.env_fps // self.model_fps) * shape[1]
        self.shapes[k] = tuple(shape)

  def reset(self) -> None:
    self.q = {k: np.zeros(self.shapes[k], dtype=self.dtypes[k]) for k in self.dtypes.keys()}

  def enqueue(self, inputs:dict[str, np.ndarray]) -> None:
    for k in inputs.keys():
      if inputs[k].dtype != self.dtypes[k]:
        raise ValueError(f'supplied input <{k}({inputs[k].dtype})> has wrong dtype, expected {self.dtypes[k]}')
      input_shape = list(self.shapes[k])
      input_shape[1] = -1
      single_input = inputs[k].reshape(tuple(input_shape))
      sz = single_input.shape[1]
      self.q[k][:,:-sz] = self.q[k][:,sz:]
      self.q[k][:,-sz:] = single_input

  def get(self, *names) -> dict[str, np.ndarray]:
    if self.env_fps == self.model_fps:
      return {k: self.q[k] for k in names}
    else:
      out = {}
      for k in names:
        shape = self.shapes[k]
        if 'img' in k: # TODO codepath diverged between xx and op
          n_channels = shape[1] // (self.env_fps // self.model_fps + (self.n_frames_input - 1))
          out[k] = np.concatenate([self.q[k][:, s:s+n_channels] for s in np.linspace(0, shape[1] - n_channels, self.n_frames_input, dtype=int)], axis=1)
        elif 'pulse' in k: # TODO this is the only key we use InputQueues for right now
          # any pulse within interval counts
          out[k] = self.q[k].reshape((shape[0], shape[1] * self.model_fps // self.env_fps, self.env_fps // self.model_fps, -1)).max(axis=2)
        else:
          idxs = np.arange(-1, -shape[1], -self.env_fps // self.model_fps)[::-1]
          out[k] = self.q[k][:, idxs]
      return out

def _make_run_policy(vision_runner, policy_runner, cam_w, cam_h,
                     vision_features_slice, frame_skip):
  model_w, model_h = MEDMODEL_INPUT_SIZE
  frame_warp = make_frame_prepare(cam_w, cam_h, model_w, model_h)

  def update_bufs(frame, img_q, tfm):
    new = frame_warp(frame, tfm).to(Device.DEFAULT)
    new_img_q = img_q[6:].cat(new, dim=0).contiguous() # TODO do we need contiguous? why
    img_pair = Tensor.cat(img_q[:6], img_q[-6:], dim=0).reshape(1, 12, model_h//2, model_w//2).contiguous()
    return new_img_q, img_pair

  def run_policy(img_q, big_img_q, frame, big_frame, tfm, big_tfm,
                 feat_q, des_buf, traf):
    tfm, big_tfm = tfm.to(WARP_DEVICE), big_tfm.to(WARP_DEVICE)

    # warp + copy in + enqueue + sample queue
    img_q, img = update_bufs(frame, img_q, tfm)
    big_img_q, big_img = update_bufs(big_frame, big_img_q, big_tfm)

    vision_out = next(iter(vision_runner({'img': img, 'big_img': big_img}).values())).cast('float32')

    des_buf, traf = des_buf.to(Device.DEFAULT), traf.to(Device.DEFAULT)
    feat_q = feat_q[:, 1:].cat(vision_out[:, vision_features_slice].reshape(1, 1, -1), dim=1).contiguous()
    feat_buf = Tensor.cat(*[feat_q[:, i:i+1] for i in range(frame_skip - 1, feat_q.shape[1], frame_skip)], dim=1) # TODO double check this, why complicated?

    policy_out = next(iter(policy_runner({
      'features_buffer': feat_buf, 'desire_pulse': des_buf, 'traffic_convention': traf,
    }).values())).cast('float32')

    # only return vision output without hidden features (they stay on device in feat_q)
    # TODO make it so we don't assume hidden state is at the end
    return img_q, big_img_q, feat_q, vision_out[:, :vision_features_slice.start].contiguous(), policy_out
  return run_policy


def compile_policy(cam_w, cam_h):
  from tinygrad.nn.onnx import OnnxRunner
  print(f"Compiling combined policy JIT for {cam_w}x{cam_h}...")

  vision_runner = OnnxRunner(MODELS_DIR / f'{MODEL_PREFIX}driving_vision.onnx')
  policy_runner = OnnxRunner(MODELS_DIR / f'{MODEL_PREFIX}driving_policy.onnx')

  model = ModelState()
  _run = _make_run_policy(vision_runner, policy_runner, cam_w, cam_h,
                          model.vision_features_slice, model.frame_skip)
  run_policy = TinyJit(_run, prune=True)

  _, _, _, yuv_size = get_nv12_info(cam_w, cam_h)

  for i in range(10):
    frame_np = np.random.randint(0, 255, yuv_size, dtype=np.uint8)
    big_frame_np = np.random.randint(0, 255, yuv_size, dtype=np.uint8)
    frame = Tensor.from_blob(frame_np.ctypes.data, (yuv_size,), dtype='uint8', device=WARP_DEVICE)
    big_frame = Tensor.from_blob(big_frame_np.ctypes.data, (yuv_size,), dtype='uint8', device=WARP_DEVICE)
    Device[WARP_DEVICE].synchronize()

    model.input_queues.enqueue({'desire_pulse': np.zeros(ModelConstants.DESIRE_LEN, dtype=np.float32)})
    model.desire_np[:] = model.input_queues.get('desire_pulse')['desire_pulse']

    st = time.perf_counter()
    outs = run_policy(
      model.img_queues['img'], model.img_queues['big_img'],
      frame, big_frame,
      model.transforms['img'], model.transforms['big_img'],
      model.features_queue,
      model.desire_tensor, model.traffic_tensor,
    )
    model.img_queues['img'], model.img_queues['big_img'] = outs[0], outs[1]
    model.features_queue = outs[2]
    Device.default.synchronize()
    t_jit = time.perf_counter()
    vision_output = outs[3].uop.base.buffer.numpy().flatten()
    t_vis = time.perf_counter()
    policy_output = outs[4].uop.base.buffer.numpy().flatten()
    t_pol = time.perf_counter()
    print(f"  [{i+1}/10] jit {(t_jit-st)*1e3:.1f} ms  vision_np {(t_vis-t_jit)*1e3:.1f} ms  policy_np {(t_pol-t_vis)*1e3:.1f} ms  total {(t_pol-st)*1e3:.1f} ms")

  pkl_path = policy_pkl_path(cam_w, cam_h)
  with open(pkl_path, 'wb') as f:
    pickle.dump(run_policy, f)
  print(f"Saved to {pkl_path}")


class ModelState:
  def __init__(self):
    with open(VISION_METADATA_PATH, 'rb') as f:
      vision_metadata = pickle.load(f)
      self.vision_input_shapes =  vision_metadata['input_shapes']
      self.vision_input_names = list(self.vision_input_shapes.keys())
      self.vision_output_slices = vision_metadata['output_slices']
      self.vision_features_slice = self.vision_output_slices.pop('hidden_state')
      vision_output_size = self.vision_features_slice.start

    with open(POLICY_METADATA_PATH, 'rb') as f:
      policy_metadata = pickle.load(f)
      self.policy_input_shapes =  policy_metadata['input_shapes']
      self.policy_output_slices = policy_metadata['output_slices']
      policy_output_size = policy_metadata['output_shapes']['outputs'][1]

    self.prev_desire = np.zeros(ModelConstants.DESIRE_LEN, dtype=np.float32)

    # desire: managed by InputQueues, shape matches policy input after downsampling
    self.input_queues = InputQueues(ModelConstants.MODEL_CONTEXT_FREQ, ModelConstants.MODEL_RUN_FREQ, ModelConstants.N_FRAMES)
    self.input_queues.update_dtypes_and_shapes({'desire_pulse': np.float32}, {'desire_pulse': self.policy_input_shapes['desire_pulse']})
    self.input_queues.reset()
    self.desire_np = np.zeros(self.policy_input_shapes['desire_pulse'], dtype=np.float32)
    self.desire_tensor = Tensor(self.desire_np, device='NPY').realize()

    # traffic convention: direct pass-through, no queue
    self.traffic_np = np.zeros(self.policy_input_shapes['traffic_convention'], dtype=np.float32)
    self.traffic_tensor = Tensor(self.traffic_np, device='NPY').realize() # TODO they stay synced?

    self.frame_skip = ModelConstants.MODEL_RUN_FREQ // ModelConstants.MODEL_CONTEXT_FREQ
    fb = self.policy_input_shapes['features_buffer']
    self.features_queue = Tensor.zeros(fb[0], fb[1] * self.frame_skip, fb[2]).contiguous().realize()

    self.img_queues = {'img': Tensor.zeros(IMG_QUEUE_SHAPE, dtype='uint8').contiguous().realize(),
                       'big_img': Tensor.zeros(IMG_QUEUE_SHAPE, dtype='uint8').contiguous().realize()}
    self.full_frames : dict[str, Tensor] = {}
    self._blob_cache : dict[int, Tensor] = {}
    self.transforms_np = {k: np.zeros((3,3), dtype=np.float32) for k in self.img_queues}
    self.transforms = {k: Tensor(v, device='NPY').realize() for k, v in self.transforms_np.items()}
    self.vision_output = np.zeros(vision_output_size, dtype=np.float32) # TODO why do we init this?
    self.policy_output = np.zeros(policy_output_size, dtype=np.float32)
    self.parser = Parser(ignore_missing=True)
    self.frame_buf_params : dict[str, tuple[int, int, int, int]] = {}
    self.run_policy = None
    self.dummy_ll_outputs = {
      'lane_lines': np.zeros((1, ModelConstants.NUM_LANE_LINES, ModelConstants.IDX_N, ModelConstants.LANE_LINES_WIDTH), dtype=np.float32),
      'lane_lines_stds': np.zeros((1, ModelConstants.NUM_LANE_LINES, ModelConstants.IDX_N, ModelConstants.LANE_LINES_WIDTH), dtype=np.float32),
      'lane_lines_prob': np.zeros((1, 8), dtype=np.float32),
      'road_edges': np.zeros((1, ModelConstants.NUM_ROAD_EDGES, ModelConstants.IDX_N, ModelConstants.LANE_LINES_WIDTH), dtype=np.float32),
      'road_edges_stds': np.zeros((1, ModelConstants.NUM_ROAD_EDGES, ModelConstants.IDX_N, ModelConstants.LANE_LINES_WIDTH), dtype=np.float32),
    }

  def slice_outputs(self, model_outputs: np.ndarray, output_slices: dict[str, slice]) -> dict[str, np.ndarray]:
    parsed_model_outputs = {k: model_outputs[np.newaxis, v] for k,v in output_slices.items()}
    return parsed_model_outputs

  def run(self, bufs: dict[str, VisionBuf], transforms: dict[str, np.ndarray],
                inputs: dict[str, np.ndarray], prepare_only: bool) -> dict[str, np.ndarray] | None:
    # Model decides when action is completed, so desire input is just a pulse triggered on rising edge
    inputs['desire_pulse'][0] = 0
    new_desire = np.where(inputs['desire_pulse'] - self.prev_desire > .99, inputs['desire_pulse'], 0)
    self.prev_desire[:] = inputs['desire_pulse']
    if self.run_policy is None:
      for key in bufs.keys():
        cam_w, cam_h = bufs[key].width, bufs[key].height
        self.frame_buf_params[key] = get_nv12_info(cam_w, cam_h)
      pkl_path = policy_pkl_path(cam_w, cam_h) # assumes both cams have same resolution
      with open(pkl_path, 'rb') as f:
        self.run_policy = pickle.load(f)

    for key in bufs.keys():
      ptr = bufs[key].data.ctypes.data
      yuv_size = self.frame_buf_params[key][3]
      # There is a ringbuffer of imgs, just cache tensors pointing to all of them
      if ptr not in self._blob_cache:
        self._blob_cache[ptr] = Tensor.from_blob(ptr, (yuv_size,), dtype='uint8', device=WARP_DEVICE)
      self.full_frames[key] = self._blob_cache[ptr]
      self.transforms_np[key][:, :] = transforms[key][:, :]

    if prepare_only: # moved before warp
      return None

    # update NPY-backed inputs â€” desire via InputQueues, traffic direct
    self.input_queues.enqueue({'desire_pulse': new_desire})
    self.desire_np[:] = self.input_queues.get('desire_pulse')['desire_pulse']
    self.traffic_np[:] = inputs['traffic_convention']

    # run combined warp + vision + policy
    outs = self.run_policy(
      self.img_queues['img'], self.img_queues['big_img'],
      self.full_frames['img'], self.full_frames['big_img'],
      self.transforms['img'], self.transforms['big_img'],
      self.features_queue,
      self.desire_tensor, self.traffic_tensor,
    )
    self.img_queues['img'], self.img_queues['big_img'] = outs[0], outs[1]
    self.features_queue = outs[2]
    vision_output, policy_output = outs[3], outs[4]

    # parse outputs
    self.vision_output = vision_output.uop.base.buffer.numpy().flatten()
    vision_outputs_dict = self.parser.parse_vision_outputs(self.slice_outputs(self.vision_output, self.vision_output_slices))
    for k, dummy_value in self.dummy_ll_outputs.items():
      vision_outputs_dict.setdefault(k, dummy_value)
    self.policy_output = policy_output.uop.base.buffer.numpy().flatten()
    policy_outputs_dict = self.parser.parse_policy_outputs(self.slice_outputs(self.policy_output, self.policy_output_slices))
    combined_outputs_dict = {**vision_outputs_dict, **policy_outputs_dict}
    if SEND_RAW_PRED:
      combined_outputs_dict['raw_pred'] = np.concatenate([self.vision_output.copy(), self.policy_output.copy()])

    return combined_outputs_dict


def main(demo=False):
  cloudlog.warning("modeld init")

  if not USBGPU:
    # USB GPU currently saturates a core so can't do this yet,
    # also need to move the aux USB interrupts for good timings
    config_realtime_process(7, 54)

  st = time.monotonic()
  cloudlog.warning("loading model")
  model = ModelState()
  cloudlog.warning(f"models loaded in {time.monotonic() - st:.1f}s, modeld starting")

  # visionipc clients
  while True:
    available_streams = VisionIpcClient.available_streams("camerad", block=False)
    if available_streams:
      use_extra_client = VisionStreamType.VISION_STREAM_WIDE_ROAD in available_streams and VisionStreamType.VISION_STREAM_ROAD in available_streams
      main_wide_camera = VisionStreamType.VISION_STREAM_ROAD not in available_streams
      break
    time.sleep(.1)

  vipc_client_main_stream = VisionStreamType.VISION_STREAM_WIDE_ROAD if main_wide_camera else VisionStreamType.VISION_STREAM_ROAD
  vipc_client_main = VisionIpcClient("camerad", vipc_client_main_stream, True)
  vipc_client_extra = VisionIpcClient("camerad", VisionStreamType.VISION_STREAM_WIDE_ROAD, False)
  cloudlog.warning(f"vision stream set up, main_wide_camera: {main_wide_camera}, use_extra_client: {use_extra_client}")

  while not vipc_client_main.connect(False):
    time.sleep(0.1)
  while use_extra_client and not vipc_client_extra.connect(False):
    time.sleep(0.1)

  cloudlog.warning(f"connected main cam with buffer size: {vipc_client_main.buffer_len} ({vipc_client_main.width} x {vipc_client_main.height})")
  if use_extra_client:
    cloudlog.warning(f"connected extra cam with buffer size: {vipc_client_extra.buffer_len} ({vipc_client_extra.width} x {vipc_client_extra.height})")

  # messaging
  pm = PubMaster(["modelV2", "drivingModelData", "cameraOdometry"])
  sm = SubMaster(["deviceState", "carState", "roadCameraState", "liveCalibration", "driverMonitoringState", "carControl", "liveDelay"])

  publish_state = PublishState()
  params = Params()

  # setup filter to track dropped frames
  frame_dropped_filter = FirstOrderFilter(0., 10., 1. / ModelConstants.MODEL_RUN_FREQ)
  frame_id = 0
  last_vipc_frame_id = 0
  run_count = 0

  model_transform_main = np.zeros((3, 3), dtype=np.float32)
  model_transform_extra = np.zeros((3, 3), dtype=np.float32)
  live_calib_seen = False
  buf_main, buf_extra = None, None
  meta_main = FrameMeta()
  meta_extra = FrameMeta()


  if demo:
    CP = get_demo_car_params()
  else:
    CP = messaging.log_from_bytes(params.get("CarParams", block=True), car.CarParams)
  cloudlog.info("modeld got CarParams: %s", CP.brand)

  # TODO this needs more thought, use .2s extra for now to estimate other delays
  # TODO Move smooth seconds to action function
  long_delay = CP.longitudinalActuatorDelay + LONG_SMOOTH_SECONDS
  prev_action = log.ModelDataV2.Action()

  DH = DesireHelper()

  while True:
    # Keep receiving frames until we are at least 1 frame ahead of previous extra frame
    while meta_main.timestamp_sof < meta_extra.timestamp_sof + 25000000:
      buf_main = vipc_client_main.recv()
      meta_main = FrameMeta(vipc_client_main)
      if buf_main is None:
        break

    if buf_main is None:
      cloudlog.debug("vipc_client_main no frame")
      continue

    if use_extra_client:
      # Keep receiving extra frames until frame id matches main camera
      while True:
        buf_extra = vipc_client_extra.recv()
        meta_extra = FrameMeta(vipc_client_extra)
        if buf_extra is None or meta_main.timestamp_sof < meta_extra.timestamp_sof + 25000000:
          break

      if buf_extra is None:
        cloudlog.debug("vipc_client_extra no frame")
        continue

      if abs(meta_main.timestamp_sof - meta_extra.timestamp_sof) > 10000000:
        cloudlog.error(f"frames out of sync! main: {meta_main.frame_id} ({meta_main.timestamp_sof / 1e9:.5f}),\
                         extra: {meta_extra.frame_id} ({meta_extra.timestamp_sof / 1e9:.5f})")

    else:
      # Use single camera
      buf_extra = buf_main
      meta_extra = meta_main

    sm.update(0)
    desire = DH.desire
    is_rhd = sm["driverMonitoringState"].isRHD
    frame_id = sm["roadCameraState"].frameId
    v_ego = max(sm["carState"].vEgo, 0.)
    lat_delay = sm["liveDelay"].lateralDelay + LAT_SMOOTH_SECONDS
    if sm.updated["liveCalibration"] and sm.seen['roadCameraState'] and sm.seen['deviceState']:
      device_from_calib_euler = np.array(sm["liveCalibration"].rpyCalib, dtype=np.float32)
      dc = DEVICE_CAMERAS[(str(sm['deviceState'].deviceType), str(sm['roadCameraState'].sensor))]
      model_transform_main = get_warp_matrix(device_from_calib_euler, dc.ecam.intrinsics if main_wide_camera else dc.fcam.intrinsics, False).astype(np.float32)
      model_transform_extra = get_warp_matrix(device_from_calib_euler, dc.ecam.intrinsics, True).astype(np.float32)
      live_calib_seen = True

    traffic_convention = np.zeros(2)
    traffic_convention[int(is_rhd)] = 1

    vec_desire = np.zeros(ModelConstants.DESIRE_LEN, dtype=np.float32)
    if desire >= 0 and desire < ModelConstants.DESIRE_LEN:
      vec_desire[desire] = 1

    # tracked dropped frames
    vipc_dropped_frames = max(0, meta_main.frame_id - last_vipc_frame_id - 1)
    frames_dropped = frame_dropped_filter.update(min(vipc_dropped_frames, 10))
    if run_count < 10: # let frame drops warm up
      frame_dropped_filter.x = 0.
      frames_dropped = 0.
    run_count = run_count + 1

    frame_drop_ratio = frames_dropped / (1 + frames_dropped)
    prepare_only = vipc_dropped_frames > 0
    if prepare_only:
      cloudlog.error(f"skipping model eval. Dropped {vipc_dropped_frames} frames")

    bufs = {name: buf_extra if 'big' in name else buf_main for name in model.vision_input_names}
    transforms = {name: model_transform_extra if 'big' in name else model_transform_main for name in model.vision_input_names}
    inputs:dict[str, np.ndarray] = {
      'desire_pulse': vec_desire,
      'traffic_convention': traffic_convention,
    }

    mt1 = time.perf_counter()
    model_output = model.run(bufs, transforms, inputs, prepare_only)
    mt2 = time.perf_counter()
    model_execution_time = mt2 - mt1
    print(f"Model execution time: {model_execution_time*1e3:.2f} ms")

    if model_output is not None:
      modelv2_send = messaging.new_message('modelV2')
      drivingdata_send = messaging.new_message('drivingModelData')
      posenet_send = messaging.new_message('cameraOdometry')

      action = get_action_from_model(model_output, prev_action, lat_delay + DT_MDL, long_delay + DT_MDL, v_ego)
      prev_action = action
      fill_model_msg(drivingdata_send, modelv2_send, model_output, action,
                     publish_state, meta_main.frame_id, meta_extra.frame_id, frame_id,
                     frame_drop_ratio, meta_main.timestamp_eof, model_execution_time, live_calib_seen)

      desire_state = modelv2_send.modelV2.meta.desireState
      l_lane_change_prob = desire_state[log.Desire.laneChangeLeft]
      r_lane_change_prob = desire_state[log.Desire.laneChangeRight]
      lane_change_prob = l_lane_change_prob + r_lane_change_prob
      DH.update(sm['carState'], sm['carControl'].latActive, lane_change_prob)
      modelv2_send.modelV2.meta.laneChangeState = DH.lane_change_state
      modelv2_send.modelV2.meta.laneChangeDirection = DH.lane_change_direction
      drivingdata_send.drivingModelData.meta.laneChangeState = DH.lane_change_state
      drivingdata_send.drivingModelData.meta.laneChangeDirection = DH.lane_change_direction

      fill_pose_msg(posenet_send, model_output, meta_main.frame_id, vipc_dropped_frames, meta_main.timestamp_eof, live_calib_seen)
      pm.send('modelV2', modelv2_send)
      pm.send('drivingModelData', drivingdata_send)
      pm.send('cameraOdometry', posenet_send)
    last_vipc_frame_id = meta_main.frame_id


if __name__ == "__main__":
  import argparse
  parser = argparse.ArgumentParser()
  parser.add_argument('--demo', action='store_true', help='A boolean for demo mode.')
  parser.add_argument('--compile', action='store_true', help='Compile combined policy JIT for all camera configs.')
  args = parser.parse_args()

  if args.compile:
    from openpilot.selfdrive.modeld.compile_warp import CAMERA_CONFIGS
    for cam_w, cam_h in CAMERA_CONFIGS:
      compile_policy(cam_w, cam_h)
  else:
    try:
      main(demo=args.demo)
    except KeyboardInterrupt:
      cloudlog.warning("got SIGINT")
